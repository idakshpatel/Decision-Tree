{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyFhuJTpjOmy"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "1. What is a Decision Tree, and how does it work?\n",
        "A Decision Tree is a supervised learning algorithm used for classification and regression tasks. It works by recursively splitting the dataset based on feature values, creating a tree-like structure where each internal node represents a decision rule, branches represent possible outcomes, and leaf nodes represent final predictions.\n",
        "\n",
        "2. What are impurity measures in Decision Trees?\n",
        "Impurity measures quantify how mixed or impure a dataset is at a given node. Common impurity measures include:\n",
        "\n",
        "Gini Impurity (used in CART)\n",
        "Entropy (used in ID3, C4.5)\n",
        "Variance Reduction (for regression trees)\n",
        "3. What is the mathematical formula for Gini Impurity?\n",
        "\n",
        "Gini Impurity = 1 - mean(p_i^2)\n",
        "\n",
        "4. What is the mathematical formula for Entropy?\n",
        "\n",
        "Entropy = -mean(p_i * log(p_i))\n",
        "\n",
        "5. What is Information Gain, and how is it used in Decision Trees?\n",
        "Information Gain (IG) measures the reduction in entropy when splitting a dataset using a feature.\n",
        "\n",
        "6. What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "\n",
        "Gini Impurity is computationally simpler and used in CART.\n",
        "Entropy is based on logarithms and is used in ID3 and C4.5.\n",
        "Both measure impurity, but Gini tends to prioritize dominant classes more.\n",
        "\n",
        "7. What is the mathematical explanation behind Decision Trees?\n",
        "A Decision Tree minimizes impurity (Gini/Entropy) at each split by selecting the best feature. The recursive process follows:\n",
        "\n",
        "Calculate impurity (Gini/Entropy) for each feature.\n",
        "Compute Information Gain for possible splits.\n",
        "Choose the feature with the highest IG.\n",
        "Recursively split until stopping criteria (like max depth, minimum samples per leaf) are met.\n",
        "\n",
        "8. What is Pre-Pruning in Decision Trees?\n",
        "Pre-Pruning (early stopping) limits tree growth by setting constraints like:\n",
        "\n",
        "Maximum depth\n",
        "Minimum samples per node\n",
        "Minimum information gain\n",
        "\n",
        "9. What is Post-Pruning in Decision Trees?\n",
        "Post-Pruning trims a fully grown tree by removing nodes that do not improve accuracy, using methods like:\n",
        "\n",
        "Cost complexity pruning\n",
        "Reduced error pruning\n",
        "\n",
        "10. What is the difference between Pre-pruning and Post-pruning?\n",
        "Feature\tPre-Pruning\tPost-Pruning\n",
        "When applied\tBefore training is complete\tAfter full tree growth\n",
        "Purpose\tPrevents overfitting early\tRemoves unnecessary branches\n",
        "Risk\tMay stop useful splits\tComputationally expensive\n",
        "\n",
        "11. What is a Decision Tree Regressor?\n",
        "\n",
        "A Decision Tree Regressor is used for regression tasks, where the output is continuous instead of categorical. It uses variance reduction instead of Gini/Entropy to decide splits.\n",
        "\n",
        "12. What are the advantages and disadvantages of Decision Trees?\n",
        "\n",
        "Advantages\n",
        "Simple to understand and interpret.\n",
        "Handles both numerical and categorical data.\n",
        "Requires minimal data preprocessing.\n",
        "\n",
        "Disadvantages\n",
        "Prone to overfitting without pruning.\n",
        "Biased towards features with more levels.\n",
        "\n",
        "Less stable; small changes in data can alter the tree structure.\n",
        "\n",
        "13. How does a Decision Tree handle missing values?\n",
        "\n",
        "Uses surrogate splits (alternative features when values are missing).\n",
        "Can assign probabilities to missing values based on the distribution of existing data.\n",
        "Some implementations use mean/mode imputation.\n",
        "\n",
        "14. How does a Decision Tree handle categorical features?\n",
        "\n",
        "Uses one-hot encoding or label encoding to convert categories into numeric values.\n",
        "Algorithms like C4.5 and CART can handle categorical features natively.\n",
        "\n",
        "15. What are some real-world applications of Decision Trees?\n",
        "\n",
        "Healthcare: Diagnosing diseases.\n",
        "Finance: Credit risk assessment.\n",
        "Retail: Customer segmentation.\n",
        "Manufacturing: Predictive maintenance.\n",
        "Fraud Detection: Identifying fraudulent transactions.\n",
        "\n",
        "'''"
      ]
    }
  ]
}